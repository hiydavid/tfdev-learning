{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO/eyzWvm4Md4ke+9hSZrVG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hiydavid/homl-learning/blob/main/projects/cifar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_6Qf-yEq5Bv"
      },
      "source": [
        "# CIFAR-10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BI6uLmk_q8Y_"
      },
      "source": [
        "---\n",
        "# Load libraries & data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtK5XffWq4da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e51d635-b403-44bf-dee8-bfd679722436"
      },
      "source": [
        "# load libraries\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import os\n",
        "\n",
        "# check versions\n",
        "print('matplotlib v.', mpl.__version__)\n",
        "print('numpy      v.', np.__version__)\n",
        "print('pandas     v.', pd.__version__)\n",
        "print('sklearn    v.', sklearn.__version__)\n",
        "print('tensorflow v.', tf.__version__)\n",
        "print('keras      v.', keras.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "matplotlib v. 3.2.2\n",
            "numpy      v. 1.19.5\n",
            "pandas     v. 1.1.5\n",
            "sklearn    v. 0.22.2.post1\n",
            "tensorflow v. 2.5.0\n",
            "keras      v. 2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdNAIV-kqJ7E",
        "outputId": "60e54f36-082a-402b-e6a1-0d5e192fc012"
      },
      "source": [
        "# load cifar10 data\n",
        "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "X_train = X_train_full[5000:]\n",
        "y_train = y_train_full[5000:]\n",
        "X_valid = X_train_full[:5000]\n",
        "y_valid = y_train_full[:5000]\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_valid.shape, y_valid.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 4s 0us/step\n",
            "(45000, 32, 32, 3) (45000, 1)\n",
            "(5000, 32, 32, 3) (5000, 1)\n",
            "(10000, 32, 32, 3) (10000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6WANg1uqfY5"
      },
      "source": [
        "---\n",
        "# Build model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUpCsg4Hqnz6"
      },
      "source": [
        "1). Build a DNN with 20 hidden layers of 100 neurons each (that's too many, but it's the point of this exercise). Use He initialization and the ELU activation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5mxeb8XqdkN"
      },
      "source": [
        "# function to clear session and set seed\n",
        "def clear_session(seed = 1234):\n",
        "    keras.backend.clear_session()\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "clear_session()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScqxpBXHq6dG",
        "outputId": "3d007dcb-6e4f-426b-cecd-75b1fc82ee32"
      },
      "source": [
        "# build model\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
        "for _ in range(20):\n",
        "    model.add(keras.layers.Dense(\n",
        "        100, activation=\"elu\", kernel_initializer=\"he_normal\")\n",
        "    )\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_2 (Flatten)          (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "dense_42 (Dense)             (None, 100)               307300    \n",
            "_________________________________________________________________\n",
            "dense_43 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_44 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_45 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_46 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_47 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_48 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_49 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_50 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_51 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_52 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_53 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_54 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_55 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_56 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_57 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_58 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_59 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_60 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_61 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_62 (Dense)             (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 500,210\n",
            "Trainable params: 500,210\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J9GJpwMr9pV"
      },
      "source": [
        "2). Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with `keras.datasets.cifar10.load_data()`. The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you'll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model's architecture or hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znh8LMhFsCot"
      },
      "source": [
        "# set optimizer\n",
        "loss = keras.losses.sparse_categorical_crossentropy\n",
        "optimizer = keras.optimizers.Nadam(learning_rate=5e-5)\n",
        "metrics = keras.metrics.sparse_categorical_accuracy\n",
        "\n",
        "# compile model\n",
        "model.compile(loss=loss, optimizer=optimizer, metrics=[metrics])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q05gTMkewFOg"
      },
      "source": [
        "# impletment callbacks and early stopping\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
        "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
        "    \"my_cifar10_model.h5\", \n",
        "    save_best_only=True\n",
        ")\n",
        "\n",
        "run_index = 1\n",
        "run_logdir = os.path.join(\n",
        "    os.curdir, \n",
        "    \"my_cifar10_logs\", \n",
        "    \"run_{:03d}\".format(run_index)\n",
        ")\n",
        "\n",
        "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
        "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4vx-a87slp6",
        "outputId": "5a38f4d4-f59b-4a1c-c2ac-2870e803e3c6"
      },
      "source": [
        "# fit model\n",
        "model.fit(\n",
        "    X_train, \n",
        "    y_train, \n",
        "    epochs=50,\n",
        "    validation_data=(X_valid, y_valid),\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1407/1407 [==============================] - 20s 10ms/step - loss: 4.2992 - sparse_categorical_accuracy: 0.1710 - val_loss: 2.1600 - val_sparse_categorical_accuracy: 0.2358\n",
            "Epoch 2/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 2.1107 - sparse_categorical_accuracy: 0.2330 - val_loss: 2.1271 - val_sparse_categorical_accuracy: 0.2308\n",
            "Epoch 3/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.9768 - sparse_categorical_accuracy: 0.2760 - val_loss: 2.0020 - val_sparse_categorical_accuracy: 0.2506\n",
            "Epoch 4/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.8929 - sparse_categorical_accuracy: 0.3094 - val_loss: 1.8800 - val_sparse_categorical_accuracy: 0.3094\n",
            "Epoch 5/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.8309 - sparse_categorical_accuracy: 0.3375 - val_loss: 1.8148 - val_sparse_categorical_accuracy: 0.3400\n",
            "Epoch 6/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.7712 - sparse_categorical_accuracy: 0.3576 - val_loss: 1.7763 - val_sparse_categorical_accuracy: 0.3492\n",
            "Epoch 7/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.7213 - sparse_categorical_accuracy: 0.3793 - val_loss: 1.6981 - val_sparse_categorical_accuracy: 0.3812\n",
            "Epoch 8/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.6826 - sparse_categorical_accuracy: 0.3932 - val_loss: 1.7594 - val_sparse_categorical_accuracy: 0.3710\n",
            "Epoch 9/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.6496 - sparse_categorical_accuracy: 0.4024 - val_loss: 1.6616 - val_sparse_categorical_accuracy: 0.3966\n",
            "Epoch 10/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.6253 - sparse_categorical_accuracy: 0.4147 - val_loss: 1.6268 - val_sparse_categorical_accuracy: 0.4078\n",
            "Epoch 11/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.5997 - sparse_categorical_accuracy: 0.4240 - val_loss: 1.6549 - val_sparse_categorical_accuracy: 0.4016\n",
            "Epoch 12/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.5789 - sparse_categorical_accuracy: 0.4294 - val_loss: 1.6033 - val_sparse_categorical_accuracy: 0.4230\n",
            "Epoch 13/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.5614 - sparse_categorical_accuracy: 0.4377 - val_loss: 1.6810 - val_sparse_categorical_accuracy: 0.3906\n",
            "Epoch 14/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.5363 - sparse_categorical_accuracy: 0.4444 - val_loss: 1.6147 - val_sparse_categorical_accuracy: 0.4232\n",
            "Epoch 15/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.5212 - sparse_categorical_accuracy: 0.4507 - val_loss: 1.5854 - val_sparse_categorical_accuracy: 0.4286\n",
            "Epoch 16/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.5070 - sparse_categorical_accuracy: 0.4562 - val_loss: 1.5726 - val_sparse_categorical_accuracy: 0.4386\n",
            "Epoch 17/50\n",
            "1407/1407 [==============================] - 13s 10ms/step - loss: 1.4934 - sparse_categorical_accuracy: 0.4611 - val_loss: 1.5816 - val_sparse_categorical_accuracy: 0.4352\n",
            "Epoch 18/50\n",
            "1407/1407 [==============================] - 13s 10ms/step - loss: 1.4794 - sparse_categorical_accuracy: 0.4670 - val_loss: 1.6069 - val_sparse_categorical_accuracy: 0.4212\n",
            "Epoch 19/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.4672 - sparse_categorical_accuracy: 0.4727 - val_loss: 1.5459 - val_sparse_categorical_accuracy: 0.4502\n",
            "Epoch 20/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.4500 - sparse_categorical_accuracy: 0.4780 - val_loss: 1.5633 - val_sparse_categorical_accuracy: 0.4352\n",
            "Epoch 21/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.4396 - sparse_categorical_accuracy: 0.4837 - val_loss: 1.5310 - val_sparse_categorical_accuracy: 0.4528\n",
            "Epoch 22/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.4307 - sparse_categorical_accuracy: 0.4837 - val_loss: 1.5350 - val_sparse_categorical_accuracy: 0.4480\n",
            "Epoch 23/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.4179 - sparse_categorical_accuracy: 0.4905 - val_loss: 1.5389 - val_sparse_categorical_accuracy: 0.4528\n",
            "Epoch 24/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.4035 - sparse_categorical_accuracy: 0.4938 - val_loss: 1.5279 - val_sparse_categorical_accuracy: 0.4524\n",
            "Epoch 25/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.3953 - sparse_categorical_accuracy: 0.4987 - val_loss: 1.5455 - val_sparse_categorical_accuracy: 0.4414\n",
            "Epoch 26/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.3858 - sparse_categorical_accuracy: 0.5003 - val_loss: 1.5223 - val_sparse_categorical_accuracy: 0.4580\n",
            "Epoch 27/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.3754 - sparse_categorical_accuracy: 0.5082 - val_loss: 1.5437 - val_sparse_categorical_accuracy: 0.4458\n",
            "Epoch 28/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.3642 - sparse_categorical_accuracy: 0.5082 - val_loss: 1.5249 - val_sparse_categorical_accuracy: 0.4546\n",
            "Epoch 29/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.3529 - sparse_categorical_accuracy: 0.5129 - val_loss: 1.5165 - val_sparse_categorical_accuracy: 0.4612\n",
            "Epoch 30/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.3477 - sparse_categorical_accuracy: 0.5146 - val_loss: 1.5223 - val_sparse_categorical_accuracy: 0.4632\n",
            "Epoch 31/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.3369 - sparse_categorical_accuracy: 0.5181 - val_loss: 1.4986 - val_sparse_categorical_accuracy: 0.4724\n",
            "Epoch 32/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.3269 - sparse_categorical_accuracy: 0.5223 - val_loss: 1.5368 - val_sparse_categorical_accuracy: 0.4632\n",
            "Epoch 33/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.3193 - sparse_categorical_accuracy: 0.5263 - val_loss: 1.4914 - val_sparse_categorical_accuracy: 0.4736\n",
            "Epoch 34/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.3097 - sparse_categorical_accuracy: 0.5306 - val_loss: 1.5055 - val_sparse_categorical_accuracy: 0.4722\n",
            "Epoch 35/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.3004 - sparse_categorical_accuracy: 0.5330 - val_loss: 1.5192 - val_sparse_categorical_accuracy: 0.4658\n",
            "Epoch 36/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2966 - sparse_categorical_accuracy: 0.5334 - val_loss: 1.5542 - val_sparse_categorical_accuracy: 0.4622\n",
            "Epoch 37/50\n",
            "1407/1407 [==============================] - 13s 10ms/step - loss: 1.2851 - sparse_categorical_accuracy: 0.5372 - val_loss: 1.5406 - val_sparse_categorical_accuracy: 0.4672\n",
            "Epoch 38/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.2812 - sparse_categorical_accuracy: 0.5387 - val_loss: 1.5245 - val_sparse_categorical_accuracy: 0.4696\n",
            "Epoch 39/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.2670 - sparse_categorical_accuracy: 0.5462 - val_loss: 1.5213 - val_sparse_categorical_accuracy: 0.4724\n",
            "Epoch 40/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.2642 - sparse_categorical_accuracy: 0.5428 - val_loss: 1.5247 - val_sparse_categorical_accuracy: 0.4630\n",
            "Epoch 41/50\n",
            "1407/1407 [==============================] - 13s 10ms/step - loss: 1.2546 - sparse_categorical_accuracy: 0.5501 - val_loss: 1.5378 - val_sparse_categorical_accuracy: 0.4702\n",
            "Epoch 42/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.2470 - sparse_categorical_accuracy: 0.5516 - val_loss: 1.5440 - val_sparse_categorical_accuracy: 0.4690\n",
            "Epoch 43/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.2399 - sparse_categorical_accuracy: 0.5560 - val_loss: 1.5143 - val_sparse_categorical_accuracy: 0.4744\n",
            "Epoch 44/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.2322 - sparse_categorical_accuracy: 0.5579 - val_loss: 1.5226 - val_sparse_categorical_accuracy: 0.4744\n",
            "Epoch 45/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.2246 - sparse_categorical_accuracy: 0.5606 - val_loss: 1.5098 - val_sparse_categorical_accuracy: 0.4826\n",
            "Epoch 46/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.2174 - sparse_categorical_accuracy: 0.5595 - val_loss: 1.5535 - val_sparse_categorical_accuracy: 0.4664\n",
            "Epoch 47/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.2087 - sparse_categorical_accuracy: 0.5657 - val_loss: 1.5335 - val_sparse_categorical_accuracy: 0.4800\n",
            "Epoch 48/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.2052 - sparse_categorical_accuracy: 0.5680 - val_loss: 1.5686 - val_sparse_categorical_accuracy: 0.4722\n",
            "Epoch 49/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.1973 - sparse_categorical_accuracy: 0.5693 - val_loss: 1.5142 - val_sparse_categorical_accuracy: 0.4890\n",
            "Epoch 50/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.1901 - sparse_categorical_accuracy: 0.5729 - val_loss: 1.5729 - val_sparse_categorical_accuracy: 0.4672\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9bc0136fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvGYfqSrwXuR",
        "outputId": "d2ea12f3-bfed-4108-ffa2-db27c7427682"
      },
      "source": [
        "# load best and evaluate\n",
        "model = keras.models.load_model(\"my_cifar10_model.h5\")\n",
        "model.evaluate(X_valid, y_valid)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 1s 3ms/step - loss: 1.4914 - sparse_categorical_accuracy: 0.4736\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.491390347480774, 0.47360000014305115]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2VMJ3GZw6mF"
      },
      "source": [
        "3). Now try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNMPdJzItGZS"
      },
      "source": [
        "# clear session\n",
        "clear_session()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKjaNkqvyOMS",
        "outputId": "18759222-4d2d-4a05-b93b-38e5a39e5716"
      },
      "source": [
        "# build model with batch norm\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "for _ in range(20):\n",
        "    model.add(keras.layers.Dense(100, kernel_initializer=\"he_normal\"))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "    model.add(keras.layers.Activation(\"elu\"))\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 3072)              12288     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 100)               307300    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 520,498\n",
            "Trainable params: 510,354\n",
            "Non-trainable params: 10,144\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSz5rmWRycb-"
      },
      "source": [
        "# set optimizer with different learning rate\n",
        "optimizer = keras.optimizers.Nadam(learning_rate=5e-4)\n",
        "model.compile(loss=loss, optimizer=optimizer, metrics=[metrics])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Om45O3asywHg",
        "outputId": "cf954b0a-3146-4458-da0b-4d431137033c"
      },
      "source": [
        "# implement callback and early stopping\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
        "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
        "    \"my_cifar10_bn_model.h5\", \n",
        "    save_best_only=True\n",
        ")\n",
        "\n",
        "run_index = 1\n",
        "run_logdir = os.path.join(\n",
        "    os.curdir, \n",
        "    \"my_cifar10_logs\", \n",
        "    \"run_bn_{:03d}\".format(run_index)\n",
        ")\n",
        "\n",
        "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
        "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
        "\n",
        "# fit model\n",
        "model.fit(\n",
        "    X_train, \n",
        "    y_train, \n",
        "    epochs=50,\n",
        "    validation_data=(X_valid, y_valid),\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# evaluate\n",
        "model = keras.models.load_model(\"my_cifar10_bn_model.h5\")\n",
        "model.evaluate(X_valid, y_valid)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1407/1407 [==============================] - 46s 26ms/step - loss: 1.8285 - sparse_categorical_accuracy: 0.3451 - val_loss: 1.6275 - val_sparse_categorical_accuracy: 0.4168\n",
            "Epoch 2/50\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 1.6601 - sparse_categorical_accuracy: 0.4117 - val_loss: 1.5554 - val_sparse_categorical_accuracy: 0.4428\n",
            "Epoch 3/50\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 1.5992 - sparse_categorical_accuracy: 0.4304 - val_loss: 1.5109 - val_sparse_categorical_accuracy: 0.4586\n",
            "Epoch 4/50\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 1.5455 - sparse_categorical_accuracy: 0.4483 - val_loss: 1.5264 - val_sparse_categorical_accuracy: 0.4564\n",
            "Epoch 5/50\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 1.5007 - sparse_categorical_accuracy: 0.4668 - val_loss: 1.4533 - val_sparse_categorical_accuracy: 0.4788\n",
            "Epoch 6/50\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 1.4667 - sparse_categorical_accuracy: 0.4811 - val_loss: 1.4310 - val_sparse_categorical_accuracy: 0.4856\n",
            "Epoch 7/50\n",
            "1407/1407 [==============================] - 36s 25ms/step - loss: 1.4328 - sparse_categorical_accuracy: 0.4935 - val_loss: 1.4470 - val_sparse_categorical_accuracy: 0.4870\n",
            "Epoch 8/50\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 1.4035 - sparse_categorical_accuracy: 0.5031 - val_loss: 1.3959 - val_sparse_categorical_accuracy: 0.5066\n",
            "Epoch 9/50\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 1.3831 - sparse_categorical_accuracy: 0.5102 - val_loss: 1.3862 - val_sparse_categorical_accuracy: 0.5072\n",
            "Epoch 10/50\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 1.3577 - sparse_categorical_accuracy: 0.5174 - val_loss: 1.3531 - val_sparse_categorical_accuracy: 0.5190\n",
            "Epoch 11/50\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 1.3350 - sparse_categorical_accuracy: 0.5287 - val_loss: 1.3732 - val_sparse_categorical_accuracy: 0.5152\n",
            "Epoch 12/50\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 1.3183 - sparse_categorical_accuracy: 0.5308 - val_loss: 1.3810 - val_sparse_categorical_accuracy: 0.5150\n",
            "Epoch 13/50\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 1.2985 - sparse_categorical_accuracy: 0.5401 - val_loss: 1.3471 - val_sparse_categorical_accuracy: 0.5196\n",
            "Epoch 14/50\n",
            "1407/1407 [==============================] - 36s 25ms/step - loss: 1.2761 - sparse_categorical_accuracy: 0.5507 - val_loss: 1.3504 - val_sparse_categorical_accuracy: 0.5232\n",
            "Epoch 15/50\n",
            "1407/1407 [==============================] - 36s 25ms/step - loss: 1.2519 - sparse_categorical_accuracy: 0.5578 - val_loss: 1.3660 - val_sparse_categorical_accuracy: 0.5252\n",
            "Epoch 16/50\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 1.2442 - sparse_categorical_accuracy: 0.5586 - val_loss: 1.3606 - val_sparse_categorical_accuracy: 0.5216\n",
            "Epoch 17/50\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 1.2412 - sparse_categorical_accuracy: 0.5606 - val_loss: 1.3459 - val_sparse_categorical_accuracy: 0.5330\n",
            "Epoch 18/50\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 1.2213 - sparse_categorical_accuracy: 0.5678 - val_loss: 1.3493 - val_sparse_categorical_accuracy: 0.5288\n",
            "Epoch 19/50\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 1.1930 - sparse_categorical_accuracy: 0.5795 - val_loss: 1.3310 - val_sparse_categorical_accuracy: 0.5388\n",
            "Epoch 20/50\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 1.1938 - sparse_categorical_accuracy: 0.5792 - val_loss: 1.3540 - val_sparse_categorical_accuracy: 0.5264\n",
            "Epoch 21/50\n",
            "1407/1407 [==============================] - 36s 25ms/step - loss: 1.1783 - sparse_categorical_accuracy: 0.5860 - val_loss: 1.3281 - val_sparse_categorical_accuracy: 0.5386\n",
            "Epoch 22/50\n",
            "1407/1407 [==============================] - 36s 25ms/step - loss: 1.1676 - sparse_categorical_accuracy: 0.5867 - val_loss: 1.3370 - val_sparse_categorical_accuracy: 0.5400\n",
            "Epoch 23/50\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 1.1476 - sparse_categorical_accuracy: 0.5939 - val_loss: 1.3480 - val_sparse_categorical_accuracy: 0.5388\n",
            "Epoch 24/50\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 1.1279 - sparse_categorical_accuracy: 0.6016 - val_loss: 1.3464 - val_sparse_categorical_accuracy: 0.5310\n",
            "Epoch 25/50\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 1.1281 - sparse_categorical_accuracy: 0.6036 - val_loss: 1.3577 - val_sparse_categorical_accuracy: 0.5392\n",
            "Epoch 26/50\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 1.1148 - sparse_categorical_accuracy: 0.6073 - val_loss: 1.3627 - val_sparse_categorical_accuracy: 0.5294\n",
            "Epoch 27/50\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 1.1022 - sparse_categorical_accuracy: 0.6087 - val_loss: 1.3572 - val_sparse_categorical_accuracy: 0.5410\n",
            "Epoch 28/50\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 1.0838 - sparse_categorical_accuracy: 0.6189 - val_loss: 1.3385 - val_sparse_categorical_accuracy: 0.5460\n",
            "Epoch 29/50\n",
            "1407/1407 [==============================] - 34s 24ms/step - loss: 1.0754 - sparse_categorical_accuracy: 0.6212 - val_loss: 1.3418 - val_sparse_categorical_accuracy: 0.5464\n",
            "Epoch 30/50\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 1.0721 - sparse_categorical_accuracy: 0.6246 - val_loss: 1.3531 - val_sparse_categorical_accuracy: 0.5386\n",
            "Epoch 31/50\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 1.0593 - sparse_categorical_accuracy: 0.6254 - val_loss: 1.3214 - val_sparse_categorical_accuracy: 0.5522\n",
            "Epoch 32/50\n",
            "1407/1407 [==============================] - 36s 25ms/step - loss: 1.0425 - sparse_categorical_accuracy: 0.6307 - val_loss: 1.3640 - val_sparse_categorical_accuracy: 0.5436\n",
            "Epoch 33/50\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 1.0330 - sparse_categorical_accuracy: 0.6316 - val_loss: 1.3647 - val_sparse_categorical_accuracy: 0.5400\n",
            "Epoch 34/50\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 1.0295 - sparse_categorical_accuracy: 0.6366 - val_loss: 1.3590 - val_sparse_categorical_accuracy: 0.5490\n",
            "Epoch 35/50\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 1.0197 - sparse_categorical_accuracy: 0.6392 - val_loss: 1.3722 - val_sparse_categorical_accuracy: 0.5402\n",
            "Epoch 36/50\n",
            "1407/1407 [==============================] - 34s 24ms/step - loss: 1.0119 - sparse_categorical_accuracy: 0.6426 - val_loss: 1.3809 - val_sparse_categorical_accuracy: 0.5362\n",
            "Epoch 37/50\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 1.0003 - sparse_categorical_accuracy: 0.6482 - val_loss: 1.3725 - val_sparse_categorical_accuracy: 0.5456\n",
            "Epoch 38/50\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 0.9990 - sparse_categorical_accuracy: 0.6481 - val_loss: 1.3900 - val_sparse_categorical_accuracy: 0.5298\n",
            "Epoch 39/50\n",
            "1407/1407 [==============================] - 36s 26ms/step - loss: 0.9834 - sparse_categorical_accuracy: 0.6540 - val_loss: 1.3940 - val_sparse_categorical_accuracy: 0.5270\n",
            "Epoch 40/50\n",
            "1407/1407 [==============================] - 36s 25ms/step - loss: 0.9802 - sparse_categorical_accuracy: 0.6529 - val_loss: 1.4027 - val_sparse_categorical_accuracy: 0.5346\n",
            "Epoch 41/50\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 0.9734 - sparse_categorical_accuracy: 0.6575 - val_loss: 1.3715 - val_sparse_categorical_accuracy: 0.5324\n",
            "Epoch 42/50\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 0.9603 - sparse_categorical_accuracy: 0.6620 - val_loss: 1.4191 - val_sparse_categorical_accuracy: 0.5348\n",
            "Epoch 43/50\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 0.9512 - sparse_categorical_accuracy: 0.6632 - val_loss: 1.3929 - val_sparse_categorical_accuracy: 0.5378\n",
            "Epoch 44/50\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.9416 - sparse_categorical_accuracy: 0.6685 - val_loss: 1.4069 - val_sparse_categorical_accuracy: 0.5336\n",
            "Epoch 45/50\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 0.9337 - sparse_categorical_accuracy: 0.6705 - val_loss: 1.4159 - val_sparse_categorical_accuracy: 0.5430\n",
            "Epoch 46/50\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 0.9306 - sparse_categorical_accuracy: 0.6710 - val_loss: 1.4057 - val_sparse_categorical_accuracy: 0.5394\n",
            "Epoch 47/50\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 0.9187 - sparse_categorical_accuracy: 0.6759 - val_loss: 1.4405 - val_sparse_categorical_accuracy: 0.5388\n",
            "Epoch 48/50\n",
            "1407/1407 [==============================] - 36s 25ms/step - loss: 0.9094 - sparse_categorical_accuracy: 0.6795 - val_loss: 1.4244 - val_sparse_categorical_accuracy: 0.5354\n",
            "Epoch 49/50\n",
            "1407/1407 [==============================] - 37s 26ms/step - loss: 0.9068 - sparse_categorical_accuracy: 0.6810 - val_loss: 1.4281 - val_sparse_categorical_accuracy: 0.5396\n",
            "Epoch 50/50\n",
            "1407/1407 [==============================] - 35s 25ms/step - loss: 0.8960 - sparse_categorical_accuracy: 0.6861 - val_loss: 1.4085 - val_sparse_categorical_accuracy: 0.5388\n",
            "157/157 [==============================] - 1s 4ms/step - loss: 1.3214 - sparse_categorical_accuracy: 0.5522\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.3213551044464111, 0.5522000193595886]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk5DF3Lh-C64"
      },
      "source": [
        "4). Try replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiZKq8TCy7ZF"
      },
      "source": [
        "# clear session\n",
        "clear_session()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5U9z9Msa-Jhk",
        "outputId": "880f1c64-13d7-4bf0-b9c2-c860c72b5e1e"
      },
      "source": [
        "# build and compile model\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
        "for _ in range(20):\n",
        "    model.add(keras.layers.Dense(\n",
        "        100, kernel_initializer=\"lecun_normal\", activation=\"selu\")\n",
        "    )\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 100)               307300    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 500,210\n",
            "Trainable params: 500,210\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1gmqssS-U9j"
      },
      "source": [
        "# set optimizer\n",
        "optimizer = keras.optimizers.Nadam(learning_rate=7e-4)\n",
        "model.compile(loss=loss, optimizer=optimizer, metrics=[metrics])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxE8JZQ--qoU"
      },
      "source": [
        "# implement early stopping\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
        "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
        "    \"my_cifar10_selu_model.h5\", \n",
        "    save_best_only=True\n",
        ")\n",
        "\n",
        "run_index = 1 # increment every time you train the model\n",
        "run_logdir = os.path.join(\n",
        "    os.curdir, \n",
        "    \"my_cifar10_logs\", \n",
        "    \"run_selu_{:03d}\".format(run_index)\n",
        ")\n",
        "\n",
        "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
        "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkQp9b8w-zhO"
      },
      "source": [
        "# normalize\n",
        "X_means = X_train.mean(axis=0)\n",
        "X_stds = X_train.std(axis=0)\n",
        "X_train_scaled = (X_train - X_means) / X_stds\n",
        "X_valid_scaled = (X_valid - X_means) / X_stds\n",
        "X_test_scaled = (X_test - X_means) / X_stds"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ls7NnIuL-1uv",
        "outputId": "63cbc4a5-20c8-416e-a70b-91a1e239c51f"
      },
      "source": [
        "# fit and evaluate model\n",
        "model.fit(\n",
        "    X_train_scaled, y_train, epochs=50,\n",
        "    validation_data=(X_valid_scaled, y_valid),\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "model = keras.models.load_model(\"my_cifar10_selu_model.h5\")\n",
        "model.evaluate(X_valid_scaled, y_valid)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1407/1407 [==============================] - 15s 11ms/step - loss: 1.8232 - sparse_categorical_accuracy: 0.3486 - val_loss: 1.7817 - val_sparse_categorical_accuracy: 0.3528\n",
            "Epoch 2/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 1.6566 - sparse_categorical_accuracy: 0.4127 - val_loss: 1.6896 - val_sparse_categorical_accuracy: 0.4054\n",
            "Epoch 3/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 1.5774 - sparse_categorical_accuracy: 0.4411 - val_loss: 1.6401 - val_sparse_categorical_accuracy: 0.4276\n",
            "Epoch 4/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 1.5149 - sparse_categorical_accuracy: 0.4694 - val_loss: 1.5484 - val_sparse_categorical_accuracy: 0.4568\n",
            "Epoch 5/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 1.4653 - sparse_categorical_accuracy: 0.4868 - val_loss: 1.6324 - val_sparse_categorical_accuracy: 0.4322\n",
            "Epoch 6/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 1.4217 - sparse_categorical_accuracy: 0.5053 - val_loss: 1.5366 - val_sparse_categorical_accuracy: 0.4614\n",
            "Epoch 7/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 1.3804 - sparse_categorical_accuracy: 0.5177 - val_loss: 1.5038 - val_sparse_categorical_accuracy: 0.4836\n",
            "Epoch 8/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 1.3438 - sparse_categorical_accuracy: 0.5341 - val_loss: 1.5047 - val_sparse_categorical_accuracy: 0.4792\n",
            "Epoch 9/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 1.3087 - sparse_categorical_accuracy: 0.5449 - val_loss: 1.5005 - val_sparse_categorical_accuracy: 0.4914\n",
            "Epoch 10/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2822 - sparse_categorical_accuracy: 0.5570 - val_loss: 1.4698 - val_sparse_categorical_accuracy: 0.4876\n",
            "Epoch 11/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2501 - sparse_categorical_accuracy: 0.5624 - val_loss: 1.5365 - val_sparse_categorical_accuracy: 0.4870\n",
            "Epoch 12/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2233 - sparse_categorical_accuracy: 0.5794 - val_loss: 1.5239 - val_sparse_categorical_accuracy: 0.4828\n",
            "Epoch 13/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2001 - sparse_categorical_accuracy: 0.5874 - val_loss: 1.5079 - val_sparse_categorical_accuracy: 0.4890\n",
            "Epoch 14/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1702 - sparse_categorical_accuracy: 0.5981 - val_loss: 1.4843 - val_sparse_categorical_accuracy: 0.5074\n",
            "Epoch 15/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1459 - sparse_categorical_accuracy: 0.6071 - val_loss: 1.5011 - val_sparse_categorical_accuracy: 0.4968\n",
            "Epoch 16/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1225 - sparse_categorical_accuracy: 0.6142 - val_loss: 1.4708 - val_sparse_categorical_accuracy: 0.5094\n",
            "Epoch 17/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1186 - sparse_categorical_accuracy: 0.6191 - val_loss: 1.4921 - val_sparse_categorical_accuracy: 0.4930\n",
            "Epoch 18/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0887 - sparse_categorical_accuracy: 0.6251 - val_loss: 1.5209 - val_sparse_categorical_accuracy: 0.5070\n",
            "Epoch 19/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0566 - sparse_categorical_accuracy: 0.6398 - val_loss: 1.5121 - val_sparse_categorical_accuracy: 0.4992\n",
            "Epoch 20/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0384 - sparse_categorical_accuracy: 0.6454 - val_loss: 1.5298 - val_sparse_categorical_accuracy: 0.5100\n",
            "Epoch 21/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0229 - sparse_categorical_accuracy: 0.6528 - val_loss: 1.5711 - val_sparse_categorical_accuracy: 0.5040\n",
            "Epoch 22/50\n",
            "1407/1407 [==============================] - 15s 10ms/step - loss: 1.0067 - sparse_categorical_accuracy: 0.6564 - val_loss: 1.5753 - val_sparse_categorical_accuracy: 0.4958\n",
            "Epoch 23/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.9894 - sparse_categorical_accuracy: 0.6632 - val_loss: 1.5858 - val_sparse_categorical_accuracy: 0.4974\n",
            "Epoch 24/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.9762 - sparse_categorical_accuracy: 0.6696 - val_loss: 1.5821 - val_sparse_categorical_accuracy: 0.5074\n",
            "Epoch 25/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.9567 - sparse_categorical_accuracy: 0.6771 - val_loss: 1.5861 - val_sparse_categorical_accuracy: 0.5078\n",
            "Epoch 26/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.9445 - sparse_categorical_accuracy: 0.6788 - val_loss: 1.6040 - val_sparse_categorical_accuracy: 0.5076\n",
            "Epoch 27/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.9202 - sparse_categorical_accuracy: 0.6884 - val_loss: 1.6034 - val_sparse_categorical_accuracy: 0.4988\n",
            "Epoch 28/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.9130 - sparse_categorical_accuracy: 0.6931 - val_loss: 1.5723 - val_sparse_categorical_accuracy: 0.5094\n",
            "Epoch 29/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.8924 - sparse_categorical_accuracy: 0.6992 - val_loss: 1.6658 - val_sparse_categorical_accuracy: 0.5104\n",
            "Epoch 30/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.8822 - sparse_categorical_accuracy: 0.7000 - val_loss: 1.6750 - val_sparse_categorical_accuracy: 0.5000\n",
            "157/157 [==============================] - 1s 3ms/step - loss: 1.4698 - sparse_categorical_accuracy: 0.4876\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.4697763919830322, 0.4875999987125397]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGeMWWm5_hFK"
      },
      "source": [
        "5). Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOBg_Lpb-8_r"
      },
      "source": [
        "# clear session\n",
        "clear_session()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuRr3VWmAtzC",
        "outputId": "3e5a09e6-24c9-455d-c1a7-690e83849670"
      },
      "source": [
        "# build model\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
        "for _ in range(20):\n",
        "    model.add(keras.layers.Dense(\n",
        "        100, kernel_initializer=\"lecun_normal\", activation=\"selu\")\n",
        "    )\n",
        "\n",
        "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 100)               307300    \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_36 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "alpha_dropout_1 (AlphaDropou (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_41 (Dense)             (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 500,210\n",
            "Trainable params: 500,210\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYs8UoqLBkXy"
      },
      "source": [
        "# set optimizer\n",
        "optimizer = keras.optimizers.Nadam(learning_rate=5e-4)\n",
        "model.compile(loss=loss, optimizer=optimizer, metrics=[metrics])"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h21_38lqB0Ou"
      },
      "source": [
        "# early stopping and callbacks\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
        "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
        "    \"my_cifar10_alpha_dropout_model.h5\", \n",
        "    save_best_only=True\n",
        ")\n",
        "\n",
        "run_index = 1 # increment every time you train the model\n",
        "run_logdir = os.path.join(\n",
        "    os.curdir, \n",
        "    \"my_cifar10_logs\", \n",
        "    \"run_alpha_dropout_{:03d}\".format(run_index)\n",
        ")\n",
        "\n",
        "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
        "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4JQOiHkCBPl"
      },
      "source": [
        "# normalize data\n",
        "X_means = X_train.mean(axis=0)\n",
        "X_stds = X_train.std(axis=0)\n",
        "X_train_scaled = (X_train - X_means) / X_stds\n",
        "X_valid_scaled = (X_valid - X_means) / X_stds\n",
        "X_test_scaled = (X_test - X_means) / X_stds"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0obZOymRCERw",
        "outputId": "da086b39-4619-4b94-a519-a4ffb2a8c242"
      },
      "source": [
        "# fit model and evaluate\n",
        "model.fit(\n",
        "    X_train_scaled, \n",
        "    y_train, \n",
        "    epochs=50,\n",
        "    validation_data=(X_valid_scaled, y_valid),\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "model = keras.models.load_model(\"my_cifar10_alpha_dropout_model.h5\")\n",
        "model.evaluate(X_valid_scaled, y_valid)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1407/1407 [==============================] - 70s 48ms/step - loss: 1.9006 - sparse_categorical_accuracy: 0.3252 - val_loss: 1.8142 - val_sparse_categorical_accuracy: 0.3654\n",
            "Epoch 2/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 1.6623 - sparse_categorical_accuracy: 0.4130 - val_loss: 1.6215 - val_sparse_categorical_accuracy: 0.4346\n",
            "Epoch 3/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 1.5697 - sparse_categorical_accuracy: 0.4484 - val_loss: 1.6208 - val_sparse_categorical_accuracy: 0.4350\n",
            "Epoch 4/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 1.5005 - sparse_categorical_accuracy: 0.4730 - val_loss: 1.5973 - val_sparse_categorical_accuracy: 0.4526\n",
            "Epoch 5/50\n",
            "1407/1407 [==============================] - 13s 10ms/step - loss: 1.4525 - sparse_categorical_accuracy: 0.4906 - val_loss: 1.5886 - val_sparse_categorical_accuracy: 0.4596\n",
            "Epoch 6/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.3976 - sparse_categorical_accuracy: 0.5117 - val_loss: 1.5483 - val_sparse_categorical_accuracy: 0.4704\n",
            "Epoch 7/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.3538 - sparse_categorical_accuracy: 0.5305 - val_loss: 1.5354 - val_sparse_categorical_accuracy: 0.4854\n",
            "Epoch 8/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.3187 - sparse_categorical_accuracy: 0.5426 - val_loss: 1.5662 - val_sparse_categorical_accuracy: 0.4954\n",
            "Epoch 9/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.2805 - sparse_categorical_accuracy: 0.5580 - val_loss: 1.5885 - val_sparse_categorical_accuracy: 0.4786\n",
            "Epoch 10/50\n",
            "1407/1407 [==============================] - 13s 9ms/step - loss: 1.2483 - sparse_categorical_accuracy: 0.5686 - val_loss: 1.5165 - val_sparse_categorical_accuracy: 0.4946\n",
            "Epoch 11/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2158 - sparse_categorical_accuracy: 0.5810 - val_loss: 1.6865 - val_sparse_categorical_accuracy: 0.4940\n",
            "Epoch 12/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1870 - sparse_categorical_accuracy: 0.5892 - val_loss: 1.6173 - val_sparse_categorical_accuracy: 0.4922\n",
            "Epoch 13/50\n",
            "1407/1407 [==============================] - 13s 10ms/step - loss: 1.1551 - sparse_categorical_accuracy: 0.6027 - val_loss: 1.5745 - val_sparse_categorical_accuracy: 0.5086\n",
            "Epoch 14/50\n",
            "1407/1407 [==============================] - 13s 10ms/step - loss: 1.1293 - sparse_categorical_accuracy: 0.6118 - val_loss: 1.5626 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 15/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1081 - sparse_categorical_accuracy: 0.6192 - val_loss: 1.6916 - val_sparse_categorical_accuracy: 0.4918\n",
            "Epoch 16/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0877 - sparse_categorical_accuracy: 0.6264 - val_loss: 1.6153 - val_sparse_categorical_accuracy: 0.4990\n",
            "Epoch 17/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0624 - sparse_categorical_accuracy: 0.6351 - val_loss: 1.6237 - val_sparse_categorical_accuracy: 0.5068\n",
            "Epoch 18/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0337 - sparse_categorical_accuracy: 0.6450 - val_loss: 1.6660 - val_sparse_categorical_accuracy: 0.4992\n",
            "Epoch 19/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0128 - sparse_categorical_accuracy: 0.6561 - val_loss: 1.6738 - val_sparse_categorical_accuracy: 0.4902\n",
            "Epoch 20/50\n",
            "1407/1407 [==============================] - 13s 10ms/step - loss: 0.9941 - sparse_categorical_accuracy: 0.6594 - val_loss: 1.6782 - val_sparse_categorical_accuracy: 0.5130\n",
            "Epoch 21/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.9737 - sparse_categorical_accuracy: 0.6686 - val_loss: 1.7055 - val_sparse_categorical_accuracy: 0.5124\n",
            "Epoch 22/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.9485 - sparse_categorical_accuracy: 0.6751 - val_loss: 1.7961 - val_sparse_categorical_accuracy: 0.5036\n",
            "Epoch 23/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.9367 - sparse_categorical_accuracy: 0.6816 - val_loss: 1.8140 - val_sparse_categorical_accuracy: 0.5006\n",
            "Epoch 24/50\n",
            "1407/1407 [==============================] - 13s 10ms/step - loss: 0.9185 - sparse_categorical_accuracy: 0.6883 - val_loss: 1.8630 - val_sparse_categorical_accuracy: 0.4920\n",
            "Epoch 25/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.9057 - sparse_categorical_accuracy: 0.6943 - val_loss: 1.7470 - val_sparse_categorical_accuracy: 0.4926\n",
            "Epoch 26/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.8928 - sparse_categorical_accuracy: 0.6975 - val_loss: 1.8144 - val_sparse_categorical_accuracy: 0.5054\n",
            "Epoch 27/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.9011 - sparse_categorical_accuracy: 0.6944 - val_loss: 1.7098 - val_sparse_categorical_accuracy: 0.4914\n",
            "Epoch 28/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.8836 - sparse_categorical_accuracy: 0.6995 - val_loss: 1.8743 - val_sparse_categorical_accuracy: 0.5036\n",
            "Epoch 29/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.8384 - sparse_categorical_accuracy: 0.7157 - val_loss: 1.9048 - val_sparse_categorical_accuracy: 0.5052\n",
            "Epoch 30/50\n",
            "1407/1407 [==============================] - 14s 10ms/step - loss: 0.8213 - sparse_categorical_accuracy: 0.7206 - val_loss: 1.9597 - val_sparse_categorical_accuracy: 0.5044\n",
            "157/157 [==============================] - 1s 3ms/step - loss: 1.5165 - sparse_categorical_accuracy: 0.4946\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.5165354013442993, 0.49459999799728394]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB1G9L_wCN26"
      },
      "source": [
        "# create mc alpha dropout\n",
        "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
        "    def call(self, inputs):\n",
        "        return super().call(inputs, training=True)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpKISPxsEYN_",
        "outputId": "b708e792-c91b-4281-ffbd-7d93ff8b855b"
      },
      "source": [
        "# add mc alpha dropout to model \n",
        "mc_model = keras.models.Sequential([\n",
        "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n",
        "    for layer in model.layers\n",
        "])\n",
        "\n",
        "mc_model.summary()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 100)               307300    \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_36 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "mc_alpha_dropout (MCAlphaDro (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_41 (Dense)             (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 500,210\n",
            "Trainable params: 500,210\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uddF7rKsEgV_"
      },
      "source": [
        "# utiliy functions\n",
        "def mc_dropout_predict_probas(mc_model, X, n_samples=10):\n",
        "    Y_probas = [mc_model.predict(X) for sample in range(n_samples)]\n",
        "    return np.mean(Y_probas, axis=0)\n",
        "\n",
        "def mc_dropout_predict_classes(mc_model, X, n_samples=10):\n",
        "    Y_probas = mc_dropout_predict_probas(mc_model, X, n_samples)\n",
        "    return np.argmax(Y_probas, axis=1)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8uOR6nfEmRS",
        "outputId": "337d8ddc-8c98-4db7-fed0-da019d531848"
      },
      "source": [
        "# clear session and make prediction with new model\n",
        "clear_session()\n",
        "\n",
        "y_pred = mc_dropout_predict_classes(mc_model, X_valid_scaled)\n",
        "accuracy = np.mean(y_pred == y_valid[:, 0])\n",
        "accuracy"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4968"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SThREnSZEwHj"
      },
      "source": [
        "6). Retrain your model using 1cycle scheduling and see if it improves training speed and model accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9MDPPaIE2H8"
      },
      "source": [
        "# clear session\n",
        "clear_session()"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgF8_CXxEsYA",
        "outputId": "9f713525-e0e4-4d2b-b69c-f954b9c6bf78"
      },
      "source": [
        "# build model\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
        "for _ in range(20):\n",
        "    model.add(keras.layers.Dense(\n",
        "        100, kernel_initializer=\"lecun_normal\", activation=\"selu\")\n",
        ")\n",
        "\n",
        "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 100)               307300    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "alpha_dropout (AlphaDropout) (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 500,210\n",
            "Trainable params: 500,210\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFWOpbZmFEVn"
      },
      "source": [
        "# set optimizer\n",
        "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
        "model.compile(loss=loss, optimizer=optimizer, metrics=[metrics])"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szh3dg4FFf1m"
      },
      "source": [
        "import math\n",
        "\n",
        "# learning rate\n",
        "K = keras.backend\n",
        "\n",
        "class ExponentialLearningRate(keras.callbacks.Callback):\n",
        "    def __init__(self, factor):\n",
        "        self.factor = factor\n",
        "        self.rates = []\n",
        "        self.losses = []\n",
        "    def on_batch_end(self, batch, logs):\n",
        "        self.rates.append(K.get_value(self.model.optimizer.lr))\n",
        "        self.losses.append(logs[\"loss\"])\n",
        "        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)\n",
        "\n",
        "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n",
        "    init_weights = model.get_weights()\n",
        "    iterations = math.ceil(len(X) / batch_size) * epochs\n",
        "    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n",
        "    init_lr = K.get_value(model.optimizer.lr)\n",
        "    K.set_value(model.optimizer.lr, min_rate)\n",
        "    exp_lr = ExponentialLearningRate(factor)\n",
        "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
        "                        callbacks=[exp_lr])\n",
        "    K.set_value(model.optimizer.lr, init_lr)\n",
        "    model.set_weights(init_weights)\n",
        "    return exp_lr.rates, exp_lr.losses\n",
        "\n",
        "def plot_lr_vs_loss(rates, losses):\n",
        "    plt.plot(rates, losses)\n",
        "    plt.gca().set_xscale('log')\n",
        "    plt.hlines(min(losses), min(rates), max(rates))\n",
        "    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])\n",
        "    plt.xlabel(\"Learning rate\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "\n",
        "class OneCycleScheduler(keras.callbacks.Callback):\n",
        "    def __init__(self, iterations, max_rate, start_rate=None,\n",
        "                 last_iterations=None, last_rate=None):\n",
        "        self.iterations = iterations\n",
        "        self.max_rate = max_rate\n",
        "        self.start_rate = start_rate or max_rate / 10\n",
        "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
        "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
        "        self.last_rate = last_rate or self.start_rate / 1000\n",
        "        self.iteration = 0\n",
        "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
        "        return ((rate2 - rate1) * (self.iteration - iter1)\n",
        "                / (iter2 - iter1) + rate1)\n",
        "    def on_batch_begin(self, batch, logs):\n",
        "        if self.iteration < self.half_iteration:\n",
        "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
        "        elif self.iteration < 2 * self.half_iteration:\n",
        "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
        "                                     self.max_rate, self.start_rate)\n",
        "        else:\n",
        "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
        "                                     self.start_rate, self.last_rate)\n",
        "        self.iteration += 1\n",
        "        K.set_value(self.model.optimizer.lr, rate)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "LqNUK5SCFM2x",
        "outputId": "f82a770f-8302-43b5-d4a5-8faff8d1d5ee"
      },
      "source": [
        "# find learning rate\n",
        "batch_size = 128\n",
        "rates, losses = find_learning_rate(\n",
        "    model, \n",
        "    X_train_scaled, \n",
        "    y_train, \n",
        "    epochs=1, \n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "plot_lr_vs_loss(rates, losses)\n",
        "plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 1.4])"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "352/352 [==============================] - 3s 6ms/step - loss: nan - sparse_categorical_accuracy: 0.1487\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9.999999747378752e-06,\n",
              " 9.615227699279785,\n",
              " 2.5806734561920166,\n",
              " 3.960209403719221)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXhV1dn38e+dCUKABEiQQAJhFBQEJA6IyuBEsc7WqdahDtW2Pm9ffazVtrb6tk+1rdVahxYfLWrFOrbFeQQRUTAo8yDzrBDmgIy53z/OAWNMIEB29k7273Nd5/Kcs1f2uRcx+WXttffa5u6IiEh8pYRdgIiIhEtBICIScwoCEZGYUxCIiMScgkBEJOYUBCIiMZcWdgH7Kzc314uKisIuQ0RqwZrN21mx/kt65DcnLcXCLqdBmzRpUqm751W1rd4FQVFRESUlJWGXISK14MmPFvPLf0/nrZ+fTF6zRmGX06CZ2eLqtunQkIhIzCkIRERiTkEgIhJzCgIRkZhTEIiIxJyCQEQk5hQEIiIxpyAQEYk5BYGISMwpCEREYk5BICIScwoCEZGYUxCIiMScgkBEJOYUBCIiMacgEBGJOQWBiEjMKQhERGJOQSAiEnOBBYGZNTaziWY2xcxmmNkdVbRpb2ajzexTM5tqZsOCqkdERKoW5IhgGzDE3XsDfYChZnZspTa/AJ51977ARcBDAdYjIiJVSAtqx+7uQFnyZXry4ZWbAc2Tz7OBFUHVIyIiVQt0jsDMUs1sMrAKeMvdJ1Rq8mvgUjNbBrwK3BBkPSIi8k2BBoG773L3PkABcLSZ9azU5GJghLsXAMOAJ83sGzWZ2bVmVmJmJatXrw6yZBGR2KmTs4bcfT0wGhhaadNVwLPJNh8CjYHcKr5+uLsXu3txXl5e0OWKiMRKkGcN5ZlZTvJ5JnAKMLtSsyXASck2PUgEgf7kFxGpQ4FNFgP5wONmlkoicJ5195fN7E6gxN1HATcBj5jZ/yUxcXxFcpJZRETqSJBnDU0F+lbx/u0Vns8EBgRVg4iI7JuuLBYRiTkFgYhIzCkIRERiTkEgIhJzCgIRkZhTEIiIxJyCQEQk5hQEIiIxpyAQEYk5BYGISMwpCEREYk5BICIScwoCEZGYUxCIiMScgkBEJOYUBCIiMacgEBGJOQWBiEjMKQhERGJOQSAiEnMKAhGRmFMQiIjEnIJARCTmAgsCM2tsZhPNbIqZzTCzO6ppd4GZzUy2GRlUPSIiUrW0APe9DRji7mVmlg6MM7PX3P2j3Q3MrCtwKzDA3deZWesA6xERkSoEFgTu7kBZ8mV68uGVml0DPOju65JfsyqoekREpGqBzhGYWaqZTQZWAW+5+4RKTboB3czsAzP7yMyGVrOfa82sxMxKVq9eHWTJIiKxE2gQuPsud+8DFABHm1nPSk3SgK7AIOBi4BEzy6liP8Pdvdjdi/Py8oIsWUQkdurkrCF3Xw+MBir/xb8MGOXuO9x9IfAZiWAQEZE6EuRZQ3m7/7o3s0zgFGB2pWb/JjEawMxySRwqWhBUTSIi8k1BnjWUDzxuZqkkAudZd3/ZzO4EStx9FPAGcKqZzQR2ATe7+5oAaxIRkUqCPGtoKtC3ivdvr/DcgRuTDxERCYGuLBYRiTkFgYhIzCkIRERiTkEgIhJzCgIRkZhTEIiIxJyCQEQk5hQEIiIxpyAQEYk5BYGISMwpCEREYk5BICIScwoCEZGYUxCIiMScgkBEJOYUBCIiMacgEBGJOQWBiEjMKQhERGJOQSAiEnMKAhGRmFMQiIjEXGBBYGaNzWyimU0xsxlmdsde2p5nZm5mxUHVIyIR5B52BQKkBbjvbcAQdy8zs3RgnJm95u4fVWxkZs2A/wNMCLAWEYmg8mQOpKZYuIXEXGAjAk8oS75MTz6qiv//B9wNbA2qFhGJpvLkiEA5EK5A5wjMLNXMJgOrgLfcfUKl7UcChe7+SpB1iEg07R4RmCkJwhRoELj7LnfvAxQAR5tZz93bzCwF+BNw0772Y2bXmlmJmZWsXr06uIJFpE65RgSRUCdnDbn7emA0MLTC282AnsAYM1sEHAuMqmrC2N2Hu3uxuxfn5eXVRckiUge+OjSkJAhTkGcN5ZlZTvJ5JnAKMHv3dnff4O657l7k7kXAR8CZ7l4SVE0iEi27Dw0pCMIV5IggHxhtZlOBj0nMEbxsZnea2ZkBfq6I1BO7RwTKgXAFdvqou08F+lbx/u3VtB8UVC0iEk2uEUEk6MpiEQlNebkmi6NAQSAiodEcQTQoCEQkNJojiAYFgYiExvcEgZIgTAoCEQmNo/mBKFAQiEhoyt01PxABCgIRCU25a6I4CmoUBGaWlVwbCDPrZmZnJpeWFhE5YOXumiiOgJqOCMYCjc2sHfAm8D1gRFBFiUg8uEYEkVDTIDB33wKcCzzk7t8BDg+uLBGJg/Jy12RxBNQ4CMysP/BdYPe9A1KDKUlE4kJzBNFQ0yD4CXAr8C93n2FmnUgsKy0icsA0RxANNVp0zt3fA96DPTeUKXX3/wqyMBFp+NydFB0bCl1NzxoaaWbNzSwLmA7MNLObgy1NRBo6HRqKhpoeGjrM3TcCZwOvAR1JnDkkInLAEheUhV2F1DQI0pPXDZwNjHL3HSSuDhcROWDlrnWGoqCmQfA3YBGQBYw1sw7AxqCKEpF4cI0IIqGmk8X3A/dXeGuxmQ0OpiQRiQutNRQNNZ0szjazP5lZSfJxD4nRgYjIAdNkcTTU9NDQY8Am4ILkYyPw96CKEpF40HUE0VDTm9d3dvfzKry+w8wmB1GQiMSIRgSRUNMRwZdmdvzuF2Y2APgymJJEJC40IoiGmo4IrgOeMLPs5Ot1wOXBlCQicaE5gmio0YjA3ae4e2/gCOAId+8LDNnb15hZYzObaGZTzGyGmd1RRZsbzWymmU01s3eSp6WKSExoRBAN+3WHMnffmLzCGODGfTTfBgxJBkgfYKiZHVupzadAsbsfATwP/H5/6hGR+k33I4iGg7lV5V6/e55QlnyZnnx4pTajk/c5APgIKDiIekSkntESE9FwMEGwzyUmzCw1eXbRKuAtd5+wl+ZXkVjHqKr9XLv7GobVq1cfWLUiEjm6oCwa9jpZbGabqPoXvgGZ+9q5u+8C+phZDvAvM+vp7tOr+JxLgWJgYDX7GQ4MByguLtYaRyINhNYaioa9BoG7N6uND3H39WY2GhhKYhnrPczsZODnwEB331Ybnyci9YPWGoqGgzk0tFdmlpccCWBmmcApwOxKbfqSWNDuTHdfFVQtIhJNOn00Gmp6HcGByAceN7NUEoHzrLu/bGZ3AiXuPgr4A9AUeC45PFzi7mcGWJOIRIgmi6MhsCBw96lA3yrev73C85OD+nwRiT7NEURDYIeGRET2RXME0aAgEJHQ6PTRaFAQiEhoyss1WRwFCgIRCY3WGooGBYGIhMbRiCAKFAQiEhp3J0W/hUKnb4GIhKbcwfa+fqXUAQWBiIRGcwTRoCAQkdBoiYloUBCISGh0QVk0KAhEJDS6oCwaFAQiEprycq01FAUKAhEJjVYfjQYFgYiERjevjwYFgYiEplwXlEWCvgUiEprEdQQaEYRNQSAiodGhoWhQEIhIaDRZHA0KAhEJja4sjoZ6HQQPj5nPfW9/tuf12s3b2bpjV4gVicj+0FpD0VCvg+DpiUt4dNxCdpU7W3fs4tR73+P4u0cz94tNYZcmIjWgOYJoqLdBsGnrDpas3cKmrTuZuWIjr05bSWnZdkrLtvHzf08PuzwRqQGtNRQNaWEXcKBmf/7VX/3j5pXy2vSVdMzN4rL+HbjjpZl8MK+U8fNLSU1J4dJj29O6WeMQqxWRqmiOIBoCCwIzawyMBRolP+d5d/9VpTaNgCeAfsAa4EJ3X7Svfbs705dvACC3aQb3vv0Z23eWc++FvflWz3weHD2fH438hPVbdmAGz3y8hEuO7sD5xQW0y8n8xv7+OXEJU5at55ffPowmGfU2G0XqHc0RREOQv/W2AUPcvczM0oFxZvaau39Uoc1VwDp372JmFwF3Axfua8e/eWUWj45bSEZqCiOuPJq7X59Nz3bZnNO3AIDrBnbiN6/MonubZtxzQW9u/88M7nvnM/7y7lx+OKgzO8qd8fNKWbtlO80apTNz5UYAFqzezMhrjiVVY1WROlHuWnQuCgILAnd3oCz5Mj358ErNzgJ+nXz+PPCAmVnya6v17uxVANx0ajd6tsvmyauO+dr2S45pz5g5q/nBwE4c3jabF64/juXrv+TOl2Zw/7vzSDE4qqgl/dq3YO2WHZyXX8CRHXL4+b+mc/0/JnFitzzOO7KAtVu207RRGtmZ6Qf1byEiVdMcQTQEehzEzFKBSUAX4EF3n1CpSTtgKYC77zSzDUAroLTSfq4FrgUobN+B0tLN/Pep3fjBwM5Vfm6TjDT+cfXXw6FdTiZ/vbQfS9d+SYusdJo1/vovd3dn2rIN/Hvyct6c+QW/SE44m0G31s3oV9SCy/p3oHub5gCUlztL1m6hoEUmaamJOfcla7YwYeEaTj8in7lflFG2bSdNMlI5vG02GWn1dl5eJDC6H0E0BBoE7r4L6GNmOcC/zKynu+/3KT3uPhwYDtC9Vx/fCvQuzNnvesyM9q2aVLvtrvOO4Hfn9uKDeWuYsmw9zRunsW7LDiYtXseoySt4rmQplx7bgYHd8rjzpZksKN1My6wMhvZsw8LVm5m7qozSsm388j/T2bqjfM++C1pk8t1jOnBY2+ac2DVXQ2GRJE0WR0OdzIy6+3ozGw0MBSoGwXKgEFhmZmlANolJ42pt2b6LFOCIgv0PgpowM47vmsvxXXO/9v7azdu567VZPPHhYv7+wSLaZjfmV2ccxjuzVjFywhK6t2lGm+xGXHJ0IdOWb+D8foXkNs1g1aZt/PHNOdz9+mwAeuQ35/pBnRnQuRVzPt/EyIlLWL1pG8d1zqW4qAX9O7UiRWNliQlNFkdDkGcN5QE7kiGQCZxCYjK4olHA5cCHwPnAu/uaH9j45Q765zev8+P2LbMy+P35vfnx4K78Z/Jyzi8uID87kyuOK2LrjnIyM1Kr/dphvfIp27qTN2d+zsPvzee/nv70a/staJHJvckrpM/p2467zutFo7Tq9yfSUOiCsmgIckSQDzyenCdIAZ5195fN7E6gxN1HAY8CT5rZPGAtcNG+dvrljl2c368gwLL3rn2rJtxwUtc9r81sryEAkJpiZDdJ5zvFhZx7ZAFjP1vNwtLNtM3J5MRuuTTJSGP1pm2MnLCEe9/+jFkrN3LPBb05vG120N0RCZUWnYsG28cf4JHTOL+rL587nVZNG4VdSiBGz17FLS9MpbRsGyf3OIT/Pu1Quh3SLOyyRALR/ZevcXn/Im4d1iPsUho8M5vk7sVVbat3V0+1y8lssCEAMLh7a974yYk88v4CnvxoMafdN5aiVll8sXEr7XIy+cHAzhzfJZc22bpSWuo/XUcQDfUuCFpkZYRdQuBaZGXw06HdufqETowYv4iZKzYw+NDWjJmziv9+bgqN0lI4q09b+hS24PQj8nWdg9Rbuo4gGupdEMRJy6wMbjyl257Xtw3rzuzPNzF87ALemvkFz5Ys467XZnFZ/yJOPuwQeuQ30ySz1Cs6fTQaFAT1SFpqCj3bZXP/xX1xd6Yu28ADo+fx0Jh5PDB6HmkpRrPGafRsl01RqywuPKqQnu004SzRpcniaFAQ1FNmRu/CHB65rJhVG7cyafE6pq/YwNrN25m0eB2TFq/jqQmLGXxoazIzUumYm8VZfdrRpXXTsEsX2cM1RxAJCoIGoHXzxnyrVz7f6pW/570NX+7g4THzeXnqClJTjNemf85f3p3H0R1bcnn/Iob1aqMfQAnV7jMWdWgofAqCBio7M52ffas7P/tWdwBKy7bxXMkynp64hB+N/IQTuubyP+f0orBl1UtuiAStPHnmug4NhU9BEBO5TRtx/aDO/ODEToycuIS7XpvNkHvGcNrhbejfuRVHFbWkS15TLW8hdaZ894hA/8+FTkEQMykpxqXHdmBI99b89b35vDrtc16euhKATrlZXH1CJ4b1akNOk4Z/mq6Ea3cQ6MhQ+BQEMdU2J5M7z+rJHWcezqI1W5iwYA0jxi/itn9N41ejpnNi1zyO7NCCbx+RT4dWWWGXKw3Q7kUNDCVB2BQEMWdmdMzNomNu4nTT6cs38tLUFbwydSXvzF7FQ6Pn8aMhXbj02A40b6wL16T27Dk0pBwInYJA9jAzehVk06sgm9uG9WD5+i/5+b+m8fvX5/Dw6Pmc16+AS4/toFNQpVZ8NVmsJAibgkCq1S4nkxFXHs305RsYPnYBT01YzIjxizi9Vz6XHNOe4zq30imocsA0RxAdCgLZp91XM5eWHcbj4xfx2LiFvDJtJUd3bMnpvfI5sVseHXM1jyD7x5M38dOIIHwKAqmx3KaNuOnUQ/nR4C48W7KUv723gF+NmkGKwXUDO3Px0e11XYLUmOYIokNBIPutcXoql/Uv4rL+RSxdu4V73/qMh8bM56Ex8+mR35xz+7bj8uOKyEhLCbtUiTBdRxAd+kmVg1LYsgl/urAPY28ezC9O70FWRiq/fXUWZ/xlHB/OX0N9u/GR1J3dk8WaZwqfgkBqRftWTbj6hE48f/1x/O9lxWzcuoOLH/mI0+4by7MlS9lVrkCQr3MdGooMBYHUupMPO4R3bhrI3ef1Ij01hZ8+P5XT73+f9+euDrs0iRCdPhodCgIJRJOMNC48qj0v33A8D1zSl7JtO/neoxM596EPeObjJZRt2xl2iRIyTRZHh4JAAmVmfPuItrx940Bu//ZhbPhyB7e8MI2jfvM2f3xjDqVl28IuUULy1XUESoKw6awhqRON01P5/vEduXJAEZ8sWceI8Yv33F3t6I4tuX5QF07smqtfCjHiOjQUGYEFgZkVAk8AhwAODHf3P1dqkw38A2ifrOWP7v73oGqS8JkZ/Tq0pF+HlvxocGdenbqSFz5ZzuWPTeTYTi25oLiQk3ocQnam1jVq6Fz3I4iMIEcEO4Gb3P0TM2sGTDKzt9x9ZoU2PwJmuvsZZpYHzDGzp9x9e4B1SUR0b9Oc7m2a8+MhXRk5YTF/G7uAG5+dQmZ6Kjee0o3vHtueJhkatDZU5bpDWWQE9lPm7iuBlcnnm8xsFtAOqBgEDjSzxPGApsBaEgEiMZKRlsIVAzpyWf8ipixbz/3vzOW3r87i/nfnct6RBQzrlU+/Di1I1Z+ODYrWGoqOOvlzy8yKgL7AhEqbHgBGASuAZsCF7rtXIPna118LXAvQvn37IEuVEKWkGH3bt+CxK45i0uJ1PP7hYkZOXMKI8YsobJnJT0/rzrePyNc8QgOh00ejI/CzhsysKfAC8BN331hp82nAZKAt0Ad4wMyaV96Huw9392J3L87Lywu6ZAmZmVFc1JK/XNyXT355Cvdf3JesjDRuePpTznloPOPnl+qK5QbANSKIjEBHBGaWTiIEnnL3F6tociVwlyf+j5hnZguB7sDEIOuS+qNpozTO7N2W03vl88Iny/jjG3O45JEJtMvJZGjPNlw5oIiCFlrorj7SiCA6gjxryIBHgVnu/qdqmi0BTgLeN7NDgEOBBUHVJPVXaopxQXEhZ/Zuy6jJK3h71hc8Pn4RI8YvYlivfK45oSNHFOSEXabsB11QFh1BjggGAN8DppnZ5OR7t5E4VRR3/yvw/4ARZjYNMOAWdy8NsCap5xqnp3LBUYVccFQhK9Z/yYjxi3h6whJemrKCYzq25JoTOjGke2utaFkP6IKy6AjyrKFxsPe7Urv7CuDUoGqQhq1tTia3DevBDUO68MzHS3ls3EKufqKETnlZXHNCJ87p247G6alhlynV0AVl0aElJqTea9Y4natP6MR7Px3Mny/qQ5OMVG59cRrH3/0uT0/UukZRpUND0aGrdaTBSE9N4aw+7Tizd1s+WrCWe96cw60vTuP2/0ynb/sWHNuxJRce3Z52OZlhlyposjhKFATS4JgZ/Tu34tkf9OejhWt477PVfDR/DQ+MnseDY+Zz2uGHcNXxHenXoWXYpcaaLiiLDgWBNFgpKcZxnXM5rnMuAMvWbeHJDxfz9MQlvDrtc47t1JJbhnanT2GOJixD4FpiIjI0RyCxUdCiCbcO68FHt53Er844jNmfb+Kch8Zz1oMfMPvzytc6StB0aCg6FAQSO00y0rhyQEfeu3kwvz2nJ0vXbmHofe9z/T8mMWPFhrDLi43yck0WR4UODUlsZWem891jOjCsZz6PfbCQER8s4rXpn3Nyj9b8eEhX+hTqArUg7V4kRIflwqcRgcRei6wMbjr1UMb9bAg3ndKNksXrOPvBD/jeoxOYuHBt2OU1WDp9NDoUBCJJ2Znp3HBSV8bdMoSffas7s1Zu5IK/fciFf/uQf3+6nE1bd4RdYoOy54IyJUHodGhIpJKmjdK4bmBnLu9fxD8/XsL/vr+QnzwzmUZpKVx+XBEXHlVI57ymYZdZ72lEEB0KApFqZGakcuWAjlzev4hPl67jHx8t4ZH3FzB87AIGH5rHDwd34agiXYtwoHafNaQ5gvApCET2ISXlq/ss3zK0O89PWspjHyziO3/9kOIOLbhuYGctdHcAdKvK6NAcgch+aJPdmB8P6cq4Wwbz6zMOY+WGrVz9RAmn3TeW50qWsn3nN26wJ9XYc2OakOsQBYHIAWmSkcYVAzoy5uZB3HdhH1JTjJufn8oJv3+Xu1+fzcSFa3UXtX0oT2amRgTh06EhkYOQnprC2X3bcVaftoydW8r/JucQHh4zn66tm3J+vwLO7tuOQ5o3DrvUyNFaQ9GhIBCpBWbGwG55DOyWx+ZtO3ll2kpGTljC716bzT1vfsZ3igu4bmBnClvqtpq7aYmJ6FAQiNSyrEZpXFBcyAXFhSwq3czw9xfwXMky/vnxUs7u046hPdvQuzCb1s3iPUrYs+icDlCHTkEgEqCi3Cz+55xe3DCkC8PHLuDpiUt44ZNlABxV1IJzjyxgQOdc2reK30hBI4LoUBCI1IH87Ex+dcbh/PS07sxYsYEJC9fywifLuPXFaQB0b9OMM3q35fRe+XRo1SQW59brgrLoUBCI1KHMjFSKi1pSXNSSHw7qzJwvNjF+3hpembaSP7wxhz+8MYfClpmc2bst5/QtoEvrhnsFs25eHx0KApGQmBnd2zSne5vmfP/4jixbt4V3Z6/i7VmreHjMfB4cPZ/eBdmc1acdJ3bLpWNuU1Ib0J/Punl9dCgIRCKioEUTLutfxGX9i1i1cSv/mbyCFz9dzp0vzwSgUVoKR7ZvwSmHHcJpPdvU+3sv69BQdFh9u+iluLjYS0pKwi5DpM4sLN3MJ4vXMWPFRsbNW81nX5QBUNyhBUN7tuHUw9rUu8nm8nLn8r9PZMLCtXx828lkN0kPu6QGz8wmuXtxVdsCGxGYWSHwBHAIiXtQDHf3P1fRbhBwH5AOlLr7wKBqEqmPOuZm0TE3i/P6JV4vLN3Mq9NW8tKUFfzmlVn85pVZdMrL4vguuQzokku/Di3Ibdoo3KL34fUZn/P+3FJ+c3ZPhUAEBDYiMLN8IN/dPzGzZsAk4Gx3n1mhTQ4wHhjq7kvMrLW7r9rbfjUiEPnK4jWbeWvmF3wwr5QJC9eyZfsuAApbZnJcp1yO75oIh5ZZGSFX+nU3PjOZ0XNWUfKLUxrUvEeUhTIicPeVwMrk801mNgtoB8ys0OwS4EV3X5Jst9cQEJGv69Aqi6tP6MTVJ3Ri+85ypixbz+Ql6/l40Vpenb6SZ0qWAnB42+YcVdSSHvnNOO3wNuQ0CS8YdpU7Yz5bzcBueQqBiKiTOQIzKwLGAj3dfWOF93cfEjocaAb82d2fqOLrrwWuBWjfvn2/xYsXB16zSH23c1c505ZvYNzcUt6fV8r05Rv2jBg65WZxREE2vQtzOKFrLl1aN6uTmpas2cIf35zDqCkr+PNFfTirT7s6+VzZ+4gg8CAws6bAe8Bv3f3FStseAIqBk4BM4EPgdHf/rLr96dCQyIFxd6Yt38D7c0uZsnQ9U5at54uN2wDITE+leWYanfOa0qcwh4IWTchtmkFOkww2b9vJpm07ad44jcz0VNZt2c7qsu2UbtrG1p27yGvaiB75zemR37zKQ1CrNm7loTHzKVm8ltkrN5GRlsJ3+hVw2+k9aJSWWtf/DLEVyqGh5AenAy8AT1UOgaRlwBp33wxsNrOxQG+g2iCYM2cOgwYNCqJckVjJBAoymrGlRWd2Nsphc1pjSlbkMX5uHqTU8Bd0+U5I+erXSOr2TaTu2LLn9a70LHalZ4HvovHGZWRtXkmzzyfz3tgy3ru3ljskByzIyWIDHgfWuvtPqmnTA3gAOA3IACYCF7n79L3sdxMw5yDLywY2HGS7qrbt673K26valguU1qC2vVH/9t1O/fvme9X1r+L76t++RbV/Hdw9r8pPc/dAHsDxJE4bnQpMTj6GAdcB11VodzOJCeTpwE9qsN+SWqht+MG2q2rbvt6rvL2qbeqf+he1/lVqo/7V4/5V9wjyrKFx1OAudO7+B+APQdVRjZdqoV1V2/b1XuXte9t2MNS/fbdT/775XnX9q82+7c/+1L/9e++A+1fvriw2sxKvZsKjIVD/6jf1r35r6P2rTn28JcTwsAsImPpXv6l/9VtD71+V6t2IQEREald9HBGIiEgtUhCIiMScgkBEJOYaVBCY2SAze9/M/ppc3rrBMbMsMysxs2+HXUttM7Meye/d82Z2fdj11DYzO9vMHjGzZ8zs1LDrqW1m1snMHjWz58OupbYkf94eT37fvht2PUGJTBCY2WNmtsrMpld6f6iZzTGzeWb2s33sxoEyoDGJ5Ssio5b6B3AL8GwwVR642uifu89y9+uAC4ABQda7v2qpf/9292tIXFR5YZD17q9a6t8Cd78q2EoP3n729Vzg+eT37cw6L7aOROasITM7kcQv8SfcvWfyvVQS6w6dQuIX+8fAxUAq8LtKu/g+iRvblJvZIcCf3D0yCV5L/esNtCIRdKXu/nLdVL9vtdE/d19lZmcC1wNPuvvIuqp/X2qrf8mvu4fE+luf1FH5+1TL/Xve3c+vq+MpcMMAAASCSURBVNr313729SzgNXefbGYj3f2SkMoOVGTuWezuY5PLVVd0NDDP3RcAmNk/gbPc/XfA3g6NrAMidYum2uhf8nBXFnAY8KWZveru5UHWXVO19f1z91HAKDN7BYhMENTS98+Au0j8YolMCECt//xF2v70lUQoFJBYIicyR1BqW2SCoBrtgKUVXi8DjqmusZmdS2IBuxwSi9lF3X71z91/DmBmV5Ac/QRa3cHb3+/fIBJD8UbAq4FWVjv2q3/ADcDJQLaZdXH3vwZZXC3Y3+9fK+C3QF8zuzUZGPVFdX29H3jAzE6n9peiiIyoB8F+8cRS11Utd92guPuIsGsIgruPAcaEXEZg3P1+Er9YGiR3X0Ni/qPB8MQS+VeGXUfQoj7UWQ4UVnhdkHyvoVD/6jf1r+GIU1+/IepB8DHQ1cw6mlkGcBEwKuSaapP6V7+pfw1HnPr6DZEJAjN7msStKg81s2VmdpW77wR+DLwBzAKedfcZYdZ5oNQ/9S/KGnr/KopTX2sqMqePiohIOCIzIhARkXAoCEREYk5BICIScwoCEZGYUxCIiMScgkBEJOYUBNJgmFlZHX/e+Dr+vBwz+2FdfqbEg4JApBpmtte1uNz9uDr+zBxAQSC1TkEgDZqZdTaz181skiXuXtc9+f4ZZjbBzD41s7eT97DAzH5tZk+a2QfAk8nXj5nZGDNbYGb/VWHfZcn/Dkpuf97MZpvZU8klpzGzYcn3JpnZ/Wb2jXtImNkVZjbKzN4F3jGzpmb2jpl9YmbTzOysZNO7gM5mNtnM/pD82pvN7GMzm2pmdwT5bykNmLvroUeDeABlVbz3DtA1+fwY4N3k8xZ8dWX91cA9yee/BiYBmRVejyexNHYusAZIr/h5wCBgA4mFylJILF9wPIkbCC0FOibbPQ28XEWNV5BY9rhl8nUa0Dz5PBeYBxhQBEyv8HWnAsOT21KAl4ETw/4+6FH/Hg1qGWqRisysKXAc8FzyD3T46oZFBcAzZpYPZAALK3zpKHf/ssLrV9x9G7DNzFYBh/DNW6FOdPdlyc+dTOKXdhmwwN137/tp4Npqyn3L3dfuLh34n+SdtMpJrJV/SBVfc2ry8WnydVOgKzC2ms8QqZKCQBqyFGC9u/epYttfSNzOdFTyhji/rrBtc6W22yo830XVPzc1abM3FT/zu0Ae0M/dd5jZIhKji8oM+J27/20/P0vkazRHIA2Wu28EFprZdyBxq0gz653cnM1X681fHlAJc4BOFW6LWNMb1mcDq5IhMBjokHx/E9CsQrs3gO8nRz6YWTsza33QVUvsaEQgDUkTM6t4yOZPJP66ftjMfgGkA/8EppAYATxnZuuAd4GOtV2Mu3+ZPN3zdTPbTGLN+5p4CnjJzKYBJcDs5P7WmNkHZjadxH2PbzazHsCHyUNfZcClwKra7os0bFqGWiRAZtbU3cuSZxE9CMx193vDrkukIh0aEgnWNcnJ4xkkDvnoeL5EjkYEIiIxpxGBiEjMKQhERGJOQSAiEnMKAhGRmFMQiIjEnIJARCTm/j/zTmoeRn8cpwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXzOfan6FTdt",
        "outputId": "971397f8-6021-48e6-f6dd-2cabd0a4a953"
      },
      "source": [
        "# clear session and rebuild model\n",
        "clear_session()\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
        "for _ in range(20):\n",
        "    model.add(keras.layers.Dense(\n",
        "        100, kernel_initializer=\"lecun_normal\", activation=\"selu\")\n",
        "    )\n",
        "\n",
        "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 100)               307300    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "alpha_dropout (AlphaDropout) (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 500,210\n",
            "Trainable params: 500,210\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OS-EvxFNF2Cl"
      },
      "source": [
        "# set optimizer\n",
        "optimizer = keras.optimizers.SGD(learning_rate=1e-2)\n",
        "model.compile(loss=loss, optimizer=optimizer, metrics=[metrics])"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHpKbSwSF762",
        "outputId": "320e2a86-fc3e-46da-8a11-37cf878efca8"
      },
      "source": [
        "# fit\n",
        "n_epochs = 15\n",
        "onecycle = OneCycleScheduler(\n",
        "    math.ceil(len(X_train_scaled) / batch_size) * n_epochs, \n",
        "    max_rate=0.05)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_scaled, \n",
        "    y_train, \n",
        "    epochs=n_epochs, \n",
        "    batch_size=batch_size,\n",
        "    validation_data=(X_valid_scaled, y_valid),\n",
        "    callbacks=[onecycle]\n",
        ")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "352/352 [==============================] - 3s 6ms/step - loss: 2.0374 - sparse_categorical_accuracy: 0.2881 - val_loss: 1.7638 - val_sparse_categorical_accuracy: 0.3766\n",
            "Epoch 2/15\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 1.7452 - sparse_categorical_accuracy: 0.3834 - val_loss: 1.6550 - val_sparse_categorical_accuracy: 0.4110\n",
            "Epoch 3/15\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 1.6188 - sparse_categorical_accuracy: 0.4239 - val_loss: 1.6604 - val_sparse_categorical_accuracy: 0.4248\n",
            "Epoch 4/15\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 1.5406 - sparse_categorical_accuracy: 0.4505 - val_loss: 1.6910 - val_sparse_categorical_accuracy: 0.4210\n",
            "Epoch 5/15\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 1.4859 - sparse_categorical_accuracy: 0.4703 - val_loss: 1.6086 - val_sparse_categorical_accuracy: 0.4400\n",
            "Epoch 6/15\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 1.4454 - sparse_categorical_accuracy: 0.4874 - val_loss: 1.6938 - val_sparse_categorical_accuracy: 0.4276\n",
            "Epoch 7/15\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 1.4106 - sparse_categorical_accuracy: 0.4988 - val_loss: 1.6572 - val_sparse_categorical_accuracy: 0.4462\n",
            "Epoch 8/15\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 1.3473 - sparse_categorical_accuracy: 0.5235 - val_loss: 1.5840 - val_sparse_categorical_accuracy: 0.4796\n",
            "Epoch 9/15\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 1.2667 - sparse_categorical_accuracy: 0.5517 - val_loss: 1.5222 - val_sparse_categorical_accuracy: 0.4960\n",
            "Epoch 10/15\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 1.1941 - sparse_categorical_accuracy: 0.5748 - val_loss: 1.5271 - val_sparse_categorical_accuracy: 0.4850\n",
            "Epoch 11/15\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 1.1235 - sparse_categorical_accuracy: 0.5989 - val_loss: 1.5792 - val_sparse_categorical_accuracy: 0.4886\n",
            "Epoch 12/15\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 1.0533 - sparse_categorical_accuracy: 0.6233 - val_loss: 1.5260 - val_sparse_categorical_accuracy: 0.5032\n",
            "Epoch 13/15\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 0.9838 - sparse_categorical_accuracy: 0.6466 - val_loss: 1.5323 - val_sparse_categorical_accuracy: 0.5130\n",
            "Epoch 14/15\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 0.9169 - sparse_categorical_accuracy: 0.6727 - val_loss: 1.5761 - val_sparse_categorical_accuracy: 0.5196\n",
            "Epoch 15/15\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 0.8751 - sparse_categorical_accuracy: 0.6874 - val_loss: 1.5933 - val_sparse_categorical_accuracy: 0.5236\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xk6yepbGFX5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}